{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32961601",
   "metadata": {},
   "source": [
    "## Zuber"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735bf1cf",
   "metadata": {},
   "source": [
    "#### 1. Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd6f8d9",
   "metadata": {},
   "source": [
    "This notebook works with two datasets that contain information for ride services in the Chicago Area in the month of Nov. 2017, with some specific data for Nov. 15-16, 2017. This is a cursory glance at the data to determine the top ten companies that provided the most rides and the likewise destination neighborhoods. These findings will be bolstered by data visualization and exploratory analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4405c58d",
   "metadata": {},
   "source": [
    "##### 1.2 Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d1fcd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries that might be necessary\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import statsmodels.api as sm\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Import DataFrams\n",
    "    #DataFraame with company name and total rides\n",
    "comp_count_trip = 'https://raw.githubusercontent.com/DHE42/zuber/refs/heads/main/moved_project_sql_result_01.csv'\n",
    "comp_count_trip_df = pd.read_csv(comp_count_trip)\n",
    "\n",
    "    #DataFrame with both destination and corresponding ride average\n",
    "dropoff_trip_avg = 'https://raw.githubusercontent.com/DHE42/zuber/refs/heads/main/moved_project_sql_result_04.csv'\n",
    "dropoff_trip_avg_df = pd.read_csv(dropoff_trip_avg)\n",
    "\n",
    "    #DataFrame with date, weather, and ride duration for Nov. 2017\n",
    "loop_ohare = 'https://raw.githubusercontent.com/DHE42/zuber/refs/heads/main/moved_project_sql_result_07.csv'\n",
    "loop_ohare_df = pd.read_csv(loop_ohare)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee64a06f",
   "metadata": {},
   "source": [
    "Above, I have imported necessary libraries and the three datasets I'll be working with. Below, I will review the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dce7ca",
   "metadata": {},
   "source": [
    "#### 2. Data Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf7114c",
   "metadata": {},
   "source": [
    "##### 2.1 Review of comp_count_trip_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47232bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head of comp_count_trip_df\n",
      "\n",
      "                      company_name  trips_amount\n",
      "0                        Flash Cab         19558\n",
      "1        Taxi Affiliation Services         11422\n",
      "2                 Medallion Leasin         10367\n",
      "3                       Yellow Cab          9888\n",
      "4  Taxi Affiliation Service Yellow          9299\n",
      "\n",
      "Info of comp_count_trip_df\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 64 entries, 0 to 63\n",
      "Data columns (total 2 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   company_name  64 non-null     object\n",
      " 1   trips_amount  64 non-null     int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 1.1+ KB\n",
      "None\n",
      "\n",
      "Description of comp_count_trip_df\n",
      "\n",
      "       trips_amount\n",
      "count     64.000000\n",
      "mean    2145.484375\n",
      "std     3812.310186\n",
      "min        2.000000\n",
      "25%       20.750000\n",
      "50%      178.500000\n",
      "75%     2106.500000\n",
      "max    19558.000000\n",
      "\n",
      "Null values in comp_count_trip_df\n",
      "\n",
      "company_name    0\n",
      "trips_amount    0\n",
      "dtype: int64\n",
      "\n",
      "Duplicates in comp_count_trip_df\n",
      "\n",
      "0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Review  comp_count_trip_df\n",
    "    #Data Head\n",
    "print(\"Head of comp_count_trip_df\")\n",
    "print()\n",
    "print(comp_count_trip_df.head())\n",
    "print()\n",
    "\n",
    "    #Data Info\n",
    "print(\"Info of comp_count_trip_df\")\n",
    "print()\n",
    "print(comp_count_trip_df.info())\n",
    "print()\n",
    "\n",
    "    #Data Description\n",
    "print(\"Description of comp_count_trip_df\")\n",
    "print()\n",
    "print(comp_count_trip_df.describe())\n",
    "print()\n",
    "\n",
    "    #Null Values\n",
    "print(\"Null values in comp_count_trip_df\")\n",
    "print()\n",
    "print(comp_count_trip_df.isnull().sum())\n",
    "print()\n",
    "\n",
    "    #Duplicate Rows\n",
    "print(\"Duplicates in comp_count_trip_df\")\n",
    "print()\n",
    "print(comp_count_trip_df.duplicated().sum())\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9118d9",
   "metadata": {},
   "source": [
    "The first thing that jumps out to me in the head of the data is the lack of standardization. To start, I will rename column 'company_name' to 'company'. Then I will convert the object dtype values in 'company' into string dtype and snake case, make sure they are all lower case, and then perform the requisite data cleaning operations such as removing heading or tailing spaces, etc. I will also rename the 'trips_amount' column 'trip_sum' for simplicity and accuracy. The data types for the trips appear to be appropriate, as one column is categorical and the other is numerical sans decimal precision necessity since there is no such thing as a partial trip. Separately calling the null values and duplicate rows confirms the lack of these types of data errors as originally shown using the describe() function. With a mean of about 2,145, a standard deviation of of about 3,812, and a median of 178, it is obvious that there is high variability in the dataset, and that the median is likely the best representation of ride company performance during the two days of November 15-16, 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72772c3",
   "metadata": {},
   "source": [
    "#### 2.2 Review of dropoff_avg_trip_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "15445bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head of dropoff_avg_trip_df\n",
      "  dropoff_location_name  average_trips\n",
      "0                  Loop   10727.466667\n",
      "1           River North    9523.666667\n",
      "2         Streeterville    6664.666667\n",
      "3             West Loop    5163.666667\n",
      "4                O'Hare    2546.900000\n",
      "\n",
      "Info of dropoff_avg_trip_df\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 94 entries, 0 to 93\n",
      "Data columns (total 2 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   dropoff_location_name  94 non-null     object \n",
      " 1   average_trips          94 non-null     float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 1.6+ KB\n",
      "None\n",
      "\n",
      "Description of dropoff_avg_trip_df\n",
      "\n",
      "       average_trips\n",
      "count      94.000000\n",
      "mean      599.953728\n",
      "std      1714.591098\n",
      "min         1.800000\n",
      "25%        14.266667\n",
      "50%        52.016667\n",
      "75%       298.858333\n",
      "max     10727.466667\n",
      "\n",
      "Null values in dropoff_avg_trip_df\n",
      "\n",
      "dropoff_location_name    0\n",
      "average_trips            0\n",
      "dtype: int64\n",
      "\n",
      "Duplicates in dropoff_avg_trip_df\n",
      "\n",
      "0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Review dropoff_avg_trip_df\n",
    "\n",
    "    #Data Head\n",
    "print(\"Head of dropoff_avg_trip_df\")\n",
    "print(dropoff_trip_avg_df.head())\n",
    "print()\n",
    "\n",
    "    #Data Info\n",
    "print(\"Info of dropoff_avg_trip_df\")\n",
    "print()\n",
    "print(dropoff_avg_trip_df.info())\n",
    "print()\n",
    "\n",
    "    #Data Description\n",
    "print(\"Description of dropoff_avg_trip_df\")\n",
    "print()\n",
    "print(dropoff_avg_trip_df.describe())\n",
    "print()\n",
    "\n",
    "    #Null Values\n",
    "print(\"Null values in dropoff_avg_trip_df\")\n",
    "print()\n",
    "print(dropoff_avg_trip_df.isnull().sum())\n",
    "print()\n",
    "\n",
    "    #Duplicate Rows\n",
    "print(\"Duplicates in dropoff_avg_trip_df\")\n",
    "print()\n",
    "print(dropoff_avg_trip_df.duplicated().sum())\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d631534e",
   "metadata": {},
   "source": [
    "This is extremely similar data to the previous DataFrame. I will begin with the aforementioned usual step of value standardization. Then, I'll rename the categorical column 'destination' and the numerical column 'trip_average'. The dtypes are correct for a string and numerical decimal precision, respectively. Since this DataFrame measures averages, it is permissible for the numerical values to contain floats for precision's sake. However, decimal places after the hundredths spot are superfluous. Separately calling functions to find null values and duplicate rows confirms the conclusion that these data gaps don't exist, which was first confirmed from calling the info() and describe() functions. As with the previous dataset, it is obvious that there are significant outliers and high variability.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c98efb",
   "metadata": {},
   "source": [
    "#### 2.3 Review of loop_ohare_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40d081e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head of loop_ohare_df\n",
      "              start_ts weather_conditions  duration_seconds\n",
      "0  2017-11-25 16:00:00               Good            2410.0\n",
      "1  2017-11-25 14:00:00               Good            1920.0\n",
      "2  2017-11-25 12:00:00               Good            1543.0\n",
      "3  2017-11-04 10:00:00               Good            2512.0\n",
      "4  2017-11-11 07:00:00               Good            1440.0\n",
      "\n",
      "Info of loop_ohare_df\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1068 entries, 0 to 1067\n",
      "Data columns (total 3 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   start_ts            1068 non-null   object \n",
      " 1   weather_conditions  1068 non-null   object \n",
      " 2   duration_seconds    1068 non-null   float64\n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 25.2+ KB\n",
      "None\n",
      "\n",
      "Description of loop_ohare_df\n",
      "\n",
      "       duration_seconds\n",
      "count       1068.000000\n",
      "mean        2071.731273\n",
      "std          769.461125\n",
      "min            0.000000\n",
      "25%         1438.250000\n",
      "50%         1980.000000\n",
      "75%         2580.000000\n",
      "max         7440.000000\n",
      "\n",
      "Null values in loop_ohare_df\n",
      "\n",
      "start_ts              0\n",
      "weather_conditions    0\n",
      "duration_seconds      0\n",
      "dtype: int64\n",
      "\n",
      "Duplicates in loop_ohare_df\n",
      "\n",
      "197\n"
     ]
    }
   ],
   "source": [
    "# Review loop_ohare_df\n",
    "\n",
    "   #Data Head\n",
    "print(\"Head of loop_ohare_df\")\n",
    "print(loop_ohare_df.head())\n",
    "print()\n",
    "\n",
    "   #Data Info\n",
    "print(\"Info of loop_ohare_df\")\n",
    "print()\n",
    "print(loop_ohare_df.info())\n",
    "print()\n",
    "\n",
    "   #Data Description\n",
    "print(\"Description of loop_ohare_df\")\n",
    "print()\n",
    "print(loop_ohare_df.describe())\n",
    "print()\n",
    "   #Null Values\n",
    "print(\"Null values in loop_ohare_df\")\n",
    "print()\n",
    "print(loop_ohare_df.isnull().sum())\n",
    "print()\n",
    "\n",
    "   #Duplicate Rows\n",
    "print(\"Duplicates in loop_ohare_df\")\n",
    "print()\n",
    "print(loop_ohare_df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a9c031",
   "metadata": {},
   "source": [
    "The loop_ohare_df has some more immediate issues besides simple string standardization. Firstly, 'start_ts' will be renamed 'start_time', 'weather_conditions' will simply be renamed 'weather', and 'duration_seconds' will be renamed 'trip_length'. Column 'start_time' will then be converted to datetime format, 'weather' will be cleaned, and 'trip_length' will be converted to timedelta dtype, which is perfect for time intervals and suitable for quick reference to enable easy storytelling with the data. Calling isnull() shows that there are no null values in the columns, however it appears there are 197 duplicate rows in loop_ohare_df. While there should be similar data for rides, it is highly unlikely that there is any completely identical data. Therefore, the duplicates can be dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63639812",
   "metadata": {},
   "source": [
    "#### 3. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c847d2",
   "metadata": {},
   "source": [
    "##### 3.1 Cleaning comp_count_trip_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c76692f",
   "metadata": {},
   "source": [
    "The first thing that jumps out to me in the head of the data is the lack of standardization. To start, I will rename column 'company_name' to 'company'. Then I will convert the object dtype values in 'company' into string dtype and snake case, make sure they are all lower case, and then perform the requisite data cleaning operations such as removing heading or tailing spaces, etc. I will also rename the 'trips_amount' column 'trip_sum' for simplicity and accuracy. The data types for the trips appear to be appropriate, as one column is categorical and the other is numerical sans decimal precision necessity since there is no such thing as a partial trip. Separately calling the null values and duplicate rows confirms the lack of these types of data errors as originally shown using the describe() function. With a mean of about 2,145, a standard deviation of of about 3,812, and a median of 178, it is obvious that there is high variability in the dataset, and that the median is likely the best representation of ride company performance during the two days of November 15-16, 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325c8d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe525416",
   "metadata": {},
   "source": [
    "##### 3.2 Cleaning dropoff_avg_trip_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35ecf29",
   "metadata": {},
   "source": [
    "##### 3.3 Cleaning loop_ohare_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
