{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32961601",
   "metadata": {},
   "source": [
    "## Zuber"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735bf1cf",
   "metadata": {},
   "source": [
    "#### 1. Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd6f8d9",
   "metadata": {},
   "source": [
    "This notebook works with two datasets that contain information for ride services in the Chicago Area in the month of Nov. 2017, with some specific data for Nov. 15-16, 2017. This is a cursory glance at the data to determine the top ten companies that provided the most rides and the likewise destination neighborhoods. These findings will be bolstered by data visualization and exploratory analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4405c58d",
   "metadata": {},
   "source": [
    "##### 1.2 Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1d1fcd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries that might be necessary\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import statsmodels.api as sm\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Import DataFrams\n",
    "    #DataFraame with company name and total rides\n",
    "comp_count_trip = 'https://raw.githubusercontent.com/DHE42/zuber/refs/heads/main/moved_project_sql_result_01.csv'\n",
    "comp_count_trip_df = pd.read_csv(comp_count_trip)\n",
    "\n",
    "    #DataFrame with both destination and corresponding ride average\n",
    "dropoff_trip_avg = 'https://raw.githubusercontent.com/DHE42/zuber/refs/heads/main/moved_project_sql_result_04.csv'\n",
    "dropoff_trip_avg_df = pd.read_csv(dropoff_trip_avg)\n",
    "\n",
    "    #DataFrame with date, weather, and ride duration for Nov. 2017\n",
    "loop_ohare = 'https://raw.githubusercontent.com/DHE42/zuber/refs/heads/main/moved_project_sql_result_07.csv'\n",
    "loop_ohare_df = pd.read_csv(loop_ohare)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee64a06f",
   "metadata": {},
   "source": [
    "Above, I have imported necessary libraries and the three datasets I'll be working with. Below, I will review the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dce7ca",
   "metadata": {},
   "source": [
    "#### 2. Data Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf7114c",
   "metadata": {},
   "source": [
    "##### 2.1 Review of comp_count_trip_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "47232bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head of comp_count_trip_df\n",
      "\n",
      "                      company_name  trips_amount\n",
      "0                        Flash Cab         19558\n",
      "1        Taxi Affiliation Services         11422\n",
      "2                 Medallion Leasin         10367\n",
      "3                       Yellow Cab          9888\n",
      "4  Taxi Affiliation Service Yellow          9299\n",
      "\n",
      "Info of comp_count_trip_df\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 64 entries, 0 to 63\n",
      "Data columns (total 2 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   company_name  64 non-null     object\n",
      " 1   trips_amount  64 non-null     int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 1.1+ KB\n",
      "None\n",
      "\n",
      "Description of comp_count_trip_df\n",
      "\n",
      "       trips_amount\n",
      "count     64.000000\n",
      "mean    2145.484375\n",
      "std     3812.310186\n",
      "min        2.000000\n",
      "25%       20.750000\n",
      "50%      178.500000\n",
      "75%     2106.500000\n",
      "max    19558.000000\n",
      "\n",
      "Null values in comp_count_trip_df\n",
      "\n",
      "company_name    0\n",
      "trips_amount    0\n",
      "dtype: int64\n",
      "\n",
      "Duplicates in comp_count_trip_df\n",
      "\n",
      "0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Review  comp_count_trip_df\n",
    "    #Data Head\n",
    "print(\"Head of comp_count_trip_df\")\n",
    "print()\n",
    "print(comp_count_trip_df.head())\n",
    "print()\n",
    "\n",
    "    #Data Info\n",
    "print(\"Info of comp_count_trip_df\")\n",
    "print()\n",
    "print(comp_count_trip_df.info())\n",
    "print()\n",
    "\n",
    "    #Data Description\n",
    "print(\"Description of comp_count_trip_df\")\n",
    "print()\n",
    "print(comp_count_trip_df.describe())\n",
    "print()\n",
    "\n",
    "    #Null Values\n",
    "print(\"Null values in comp_count_trip_df\")\n",
    "print()\n",
    "print(comp_count_trip_df.isnull().sum())\n",
    "print()\n",
    "\n",
    "    #Duplicate Rows\n",
    "print(\"Duplicates in comp_count_trip_df\")\n",
    "print()\n",
    "print(comp_count_trip_df.duplicated().sum())\n",
    "print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9118d9",
   "metadata": {},
   "source": [
    "The 'company_name' column displays the name of the ride company, and 'trips_amount' displays the number of rides the service has given. The first thing that jumps out to me in the head of the data is the lack of standardization. To start, I will rename column 'company_name' to 'company'. Then I will convert the object dtype values in 'company' into string dtype and snake case, make sure they are all lower case, and then perform the requisite data cleaning operations such as removing heading or tailing spaces, etc. I will also rename the 'trips_amount' column 'trip_sum' for simplicity and accuracy. The data types for the trips appear to be appropriate, as one column is categorical and the other is numerical sans decimal precision necessity since there is no such thing as a partial trip. Separately calling the null values and duplicate rows confirms the lack of these types of data errors as originally shown using the describe() function. With a mean of about 2,145, a standard deviation of of about 3,812, and a median of 178, it is obvious that there is high variability in the dataset, and that the median is likely the best representation of ride company performance during the two days of November 15-16, 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72772c3",
   "metadata": {},
   "source": [
    "#### 2.2 Review of dropoff_avg_trip_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "15445bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head of dropoff_trip_avg\n",
      "  dropoff_location_name  average_trips\n",
      "0                  Loop   10727.466667\n",
      "1           River North    9523.666667\n",
      "2         Streeterville    6664.666667\n",
      "3             West Loop    5163.666667\n",
      "4                O'Hare    2546.900000\n",
      "\n",
      "Info of dropoff_trip_avg\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 94 entries, 0 to 93\n",
      "Data columns (total 2 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   dropoff_location_name  94 non-null     object \n",
      " 1   average_trips          94 non-null     float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 1.6+ KB\n",
      "None\n",
      "\n",
      "Description of dropoff_trip_avg\n",
      "\n",
      "       average_trips\n",
      "count      94.000000\n",
      "mean      599.953728\n",
      "std      1714.591098\n",
      "min         1.800000\n",
      "25%        14.266667\n",
      "50%        52.016667\n",
      "75%       298.858333\n",
      "max     10727.466667\n",
      "\n",
      "Null values in dropoff_trip_avg\n",
      "\n",
      "dropoff_location_name    0\n",
      "average_trips            0\n",
      "dtype: int64\n",
      "\n",
      "Duplicates in dropoff_trip_avg\n",
      "\n",
      "0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Review dropoff_trip_avg\n",
    "\n",
    "    #Data Head\n",
    "print(\"Head of dropoff_trip_avg\")\n",
    "print(dropoff_trip_avg_df.head())\n",
    "print()\n",
    "\n",
    "    #Data Info\n",
    "print(\"Info of dropoff_trip_avg\")\n",
    "print()\n",
    "print(dropoff_trip_avg_df.info())\n",
    "print()\n",
    "\n",
    "    #Data Description\n",
    "print(\"Description of dropoff_trip_avg\")\n",
    "print()\n",
    "print(dropoff_trip_avg_df.describe())\n",
    "print()\n",
    "\n",
    "    #Null Values\n",
    "print(\"Null values in dropoff_trip_avg\")\n",
    "print()\n",
    "print(dropoff_trip_avg_df.isnull().sum())\n",
    "print()\n",
    "\n",
    "    #Duplicate Rows\n",
    "print(\"Duplicates in dropoff_trip_avg\")\n",
    "print()\n",
    "print(dropoff_trip_avg_df.duplicated().sum())\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d631534e",
   "metadata": {},
   "source": [
    "This is extremely similar data to the previous DataFrame. The column 'dropoff_location_name' corresponds to a neighborhood that is the destination of the ride service, and 'average_trips' refers to the mean of how many times ride ended in that neighborhood. I will begin with the aforementioned usual step of value standardization. Then, I'll rename the categorical column 'destination' and the numerical column 'trip_average'. The dtypes are correct for a string and numerical decimal precision, respectively. Since this DataFrame measures averages, it is permissible for the numerical values to contain floats for precision's sake. However, decimal places after the hundredths spot are superfluous. Separately calling functions to find null values and duplicate rows confirms the conclusion that these data gaps don't exist, which was first confirmed from calling the info() and describe() functions. As with the previous dataset, it is obvious that there are significant outliers and high variability.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c98efb",
   "metadata": {},
   "source": [
    "#### 2.3 Review of loop_ohare_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "40d081e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head of loop_ohare_df\n",
      "              start_ts weather_conditions  duration_seconds\n",
      "0  2017-11-25 16:00:00               Good            2410.0\n",
      "1  2017-11-25 14:00:00               Good            1920.0\n",
      "2  2017-11-25 12:00:00               Good            1543.0\n",
      "3  2017-11-04 10:00:00               Good            2512.0\n",
      "4  2017-11-11 07:00:00               Good            1440.0\n",
      "\n",
      "Info of loop_ohare_df\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1068 entries, 0 to 1067\n",
      "Data columns (total 3 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   start_ts            1068 non-null   object \n",
      " 1   weather_conditions  1068 non-null   object \n",
      " 2   duration_seconds    1068 non-null   float64\n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 25.2+ KB\n",
      "None\n",
      "\n",
      "Description of loop_ohare_df\n",
      "\n",
      "       duration_seconds\n",
      "count       1068.000000\n",
      "mean        2071.731273\n",
      "std          769.461125\n",
      "min            0.000000\n",
      "25%         1438.250000\n",
      "50%         1980.000000\n",
      "75%         2580.000000\n",
      "max         7440.000000\n",
      "\n",
      "Null values in loop_ohare_df\n",
      "\n",
      "start_ts              0\n",
      "weather_conditions    0\n",
      "duration_seconds      0\n",
      "dtype: int64\n",
      "\n",
      "Duplicates in loop_ohare_df\n",
      "\n",
      "197\n"
     ]
    }
   ],
   "source": [
    "# Review loop_ohare_df\n",
    "\n",
    "   #Data Head\n",
    "print(\"Head of loop_ohare_df\")\n",
    "print(loop_ohare_df.head())\n",
    "print()\n",
    "\n",
    "   #Data Info\n",
    "print(\"Info of loop_ohare_df\")\n",
    "print()\n",
    "print(loop_ohare_df.info())\n",
    "print()\n",
    "\n",
    "   #Data Description\n",
    "print(\"Description of loop_ohare_df\")\n",
    "print()\n",
    "print(loop_ohare_df.describe())\n",
    "print()\n",
    "   #Null Values\n",
    "print(\"Null values in loop_ohare_df\")\n",
    "print()\n",
    "print(loop_ohare_df.isnull().sum())\n",
    "print()\n",
    "\n",
    "   #Duplicate Rows\n",
    "print(\"Duplicates in loop_ohare_df\")\n",
    "print()\n",
    "print(loop_ohare_df.duplicated().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a9c031",
   "metadata": {},
   "source": [
    "Column 'start_ts' displays the start date and time of the ride, 'weather_conditions' displays if the weather was good or bad, and 'duration_seconds' displays how many seconds the whole trip lasted. The loop_ohare_df has some more immediate issues besides simple string standardization. Firstly, 'start_ts' will be renamed 'start_time', 'weather_conditions' will simply be renamed 'weather', and 'duration_seconds' will be renamed 'trip_length'. Column 'start_time' will then be converted to datetime format, 'weather' will be cleaned, and 'trip_length' will be divided by 60 to show minutes, as this is better for visual storytelling, as well as more readily intelligible. Calling isnull() shows that there are no null values in the columns, however it appears there are 197 duplicate rows in loop_ohare_df. While there should be similar data for rides, it is highly unlikely that there is any completely identical data. Therefore, the duplicates can be dropped.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63639812",
   "metadata": {},
   "source": [
    "#### 3. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c847d2",
   "metadata": {},
   "source": [
    "##### 3.1 Cleaning comp_count_trip_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c76692f",
   "metadata": {},
   "source": [
    "Steps to clean comp_count_trip_df:\n",
    "\n",
    "1) Rename Columns\n",
    "2) Convert Data Types\n",
    "3) Clean 'company'\n",
    "4) Call Head to Confirm Changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "325c8d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head of comp_count_trip_df after cleaning\n",
      "\n",
      "                           company  trip_sum\n",
      "0                        flash_cab     19558\n",
      "1        taxi_affiliation_services     11422\n",
      "2                 medallion_leasin     10367\n",
      "3                       yellow_cab      9888\n",
      "4  taxi_affiliation_service_yellow      9299\n"
     ]
    }
   ],
   "source": [
    "# Renaming Columns\n",
    "\n",
    "    # Rename 'company_name' to 'company' and 'trip_amounts' to 'trip_sum'\n",
    "comp_count_trip_df.rename(columns={'company_name': 'company', 'trips_amount': 'trip_sum'}, inplace=True)\n",
    "\n",
    "# Dtype Conversion\n",
    "   \n",
    "    # Convert 'company' to string\n",
    "comp_count_trip_df['company'] = comp_count_trip_df['company'].astype('str')\n",
    "\n",
    "# Cleaning the 'company' column\n",
    "\n",
    "    # Remove leading and trailing spaces\n",
    "comp_count_trip_df['company'] = comp_count_trip_df['company'].str.strip()\n",
    "\n",
    "    # Replace ampersand with 'and'\n",
    "comp_count_trip_df['company'] = comp_count_trip_df['company'].str.replace('&', 'and')\n",
    "\n",
    "    # Remove periods, commas, hyphens, and apostraphes\n",
    "comp_count_trip_df['company'] = comp_count_trip_df['company'].str.replace(r'[.,\\'-]', '', regex=True)\n",
    "\n",
    "    # Convert to lowercase\n",
    "comp_count_trip_df['company'] = comp_count_trip_df['company'].str.lower()\n",
    "\n",
    "# Replace spaces with underscores\n",
    "comp_count_trip_df['company'] = comp_count_trip_df['company'].str.replace(' ', '_')\n",
    "\n",
    "# Call head to confirm changes\n",
    "print(\"Head of comp_count_trip_df after cleaning\")\n",
    "print()\n",
    "print(comp_count_trip_df.head())\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe525416",
   "metadata": {},
   "source": [
    "##### 3.2 Cleaning dropoff_avg_trip_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51807228",
   "metadata": {},
   "source": [
    "Steps to clean dropoff_avg_trip_df:\n",
    "\n",
    "1) Rename columns\n",
    "2) Replace opaque neighborhood names with clear neighborhood names\n",
    "3) Round to hundredths place in categorical column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ef97533b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head of dropoff_trip_avg_df after cleaning\n",
      "\n",
      "     destination  trip_average\n",
      "0           loop      10727.47\n",
      "1    river_north       9523.67\n",
      "2  streeterville       6664.67\n",
      "3      west_loop       5163.67\n",
      "4          ohare       2546.90\n"
     ]
    }
   ],
   "source": [
    "# Rename Columns\n",
    "\n",
    "    # Rename 'dropoff_location_name' to 'destinatin' and 'average_trips' to 'trip_average\n",
    "dropoff_trip_avg_df.rename(columns={'dropoff_location_name': 'destination', 'average_trips': 'trip_average'}, inplace=True)\n",
    "\n",
    "# Dtype Conversion\n",
    "    # Convert 'destination' to string\n",
    "dropoff_trip_avg_df['destination'] = dropoff_trip_avg_df['destination'].astype('str')\n",
    "\n",
    "# Cleaning the 'destination' column\n",
    "    \n",
    "    # Remove leading and trailing spaces\n",
    "dropoff_trip_avg_df['destination'] = dropoff_trip_avg_df['destination'].str.strip()\n",
    "    \n",
    "    # Convert to lowercase\n",
    "dropoff_trip_avg_df['destination'] = dropoff_trip_avg_df['destination'].str.lower()\n",
    "    \n",
    "    # Replace spaces with underscores\n",
    "dropoff_trip_avg_df['destination'] = dropoff_trip_avg_df['destination'].str.replace(' ', '_')\n",
    "    \n",
    "    # Replace ampersands with 'and'\n",
    "dropoff_trip_avg_df['destination'] = dropoff_trip_avg_df['destination'].str.replace('&', 'and')\n",
    "    \n",
    "    # Replace 'sauganash,forest_glen' with 'forest_glen'\n",
    "dropoff_trip_avg_df['destination'] = dropoff_trip_avg_df['destination'].str.replace('sauganash,forest_glen', 'forest_glen')\n",
    "\n",
    "    # Replace 'o'hare' with 'ohare'\n",
    "dropoff_trip_avg_df['destination'] = dropoff_trip_avg_df['destination'].str.replace(\"o'hare\", 'ohare')\n",
    "\n",
    "    # Replace 'little_italy,_uic' with 'little_italy'\n",
    "dropoff_trip_avg_df['destination'] = dropoff_trip_avg_df['destination'].str.replace('little_italy,_uic', 'little_italy')\n",
    "\n",
    "    # Replace 'undef' with 'undefined'\n",
    "dropoff_trip_avg_df['destination'] = dropoff_trip_avg_df['destination'].str.replace('undef', 'undefined')\n",
    "\n",
    "# Round 'trip_avg' to hundredths spot\n",
    "dropoff_trip_avg_df['trip_average'] = dropoff_trip_avg_df['trip_average'].round(2)\n",
    "\n",
    "# Print the head of the DataFrame to confirm changes\n",
    "print(\"Head of dropoff_trip_avg_df after cleaning\")\n",
    "print()\n",
    "print(dropoff_trip_avg_df.head())\n",
    "\n",
    "# Test for duplicates\n",
    "    # Check for duplicates in 'destination' column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35ecf29",
   "metadata": {},
   "source": [
    "##### 3.3 Cleaning loop_ohare_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1876a504",
   "metadata": {},
   "source": [
    "Steps to clean loop_ohare_df:\n",
    "\n",
    "1) Rename columns\n",
    "2) Convert data types\n",
    "3) Drop duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "43bcb142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head of loop_ohare_df after cleaning\n",
      "\n",
      "           start_time weather  trip_length\n",
      "0 2017-11-25 16:00:00    good        40.17\n",
      "1 2017-11-25 14:00:00    good        32.00\n",
      "2 2017-11-25 12:00:00    good        25.72\n",
      "3 2017-11-04 10:00:00    good        41.87\n",
      "4 2017-11-11 07:00:00    good        24.00\n",
      "\n",
      "Duplicates in loop_ohare_df\n",
      "\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Rename Columns\n",
    "    # Rename 'start_ts' to 'start_time, 'weather_conditions' to 'weather', and 'duration_seconds' to 'trip_length'\n",
    "loop_ohare_df.rename(columns={'start_ts': 'start_time', 'weather_conditions': 'weather', 'duration_seconds': 'trip_length'}, inplace=True)\n",
    "\n",
    "# Dtype Conversion\n",
    "    # Convert 'start_time' to datetime\n",
    "loop_ohare_df['start_time'] = pd.to_datetime(loop_ohare_df['start_time'], format='%Y-%m-%d %H:%M:%S')\n",
    "    # Convert 'weather' to string\n",
    "loop_ohare_df['weather'] = loop_ohare_df['weather'].astype('str')\n",
    "    \n",
    "    \n",
    "\n",
    "# Cleaning the 'weather' column\n",
    "    # Remove leading and trailing spaces\n",
    "loop_ohare_df['weather'] = loop_ohare_df['weather'].str.strip()\n",
    "\n",
    "    # Convert to lowercase\n",
    "loop_ohare_df['weather'] = loop_ohare_df['weather'].str.lower()\n",
    "\n",
    "    # Replace spaces with underscores\n",
    "loop_ohare_df['weather'] = loop_ohare_df['weather'].str.replace(' ', '_')\n",
    "\n",
    "# Cleaning the 'trip_length' column\n",
    "    # Divide 'trip_length' by 60 to convert seconds to minutes\n",
    "loop_ohare_df['trip_length'] = loop_ohare_df['trip_length'] / 60\n",
    "    # Round 'trip_length' to hundredths spot\n",
    "loop_ohare_df['trip_length'] = loop_ohare_df['trip_length'].round(2)\n",
    "\n",
    "\n",
    "# Drop duplicates\n",
    "loop_ohare_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Print the head of the DataFrame to confirm changes\n",
    "print(\"Head of loop_ohare_df after cleaning\")\n",
    "print()\n",
    "print(loop_ohare_df.head())\n",
    "print()\n",
    "\n",
    "#Duplicate Rows\n",
    "print(\"Duplicates in loop_ohare_df\")\n",
    "print()\n",
    "print(loop_ohare_df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52422308",
   "metadata": {},
   "source": [
    "#### 4. Exploratory Data Analysis and Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cf74cd",
   "metadata": {},
   "source": [
    "Let's do some visualization to find out the the top ten companies who give the most rides, and the top ten ride service destinations. Since trip_average displays the mean, there will be decimals. However, it is important to consider that there is no such thing as a partial ride to the destination. It is completed or not. Therefore, the top ten will be coerced and converted to the integer dtype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6caa601e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ten Companies With Highest Usage\n",
      "\n",
      "                            company  trip_sum\n",
      "0                         flash_cab     19558\n",
      "1         taxi_affiliation_services     11422\n",
      "2                  medallion_leasin     10367\n",
      "3                        yellow_cab      9888\n",
      "4   taxi_affiliation_service_yellow      9299\n",
      "5         chicago_carriage_cab_corp      9181\n",
      "6                      city_service      8448\n",
      "7                          sun_taxi      7701\n",
      "8         star_north_management_llc      7455\n",
      "9  blue_ribbon_taxi_association_inc      5953\n",
      "\n",
      "Ten Destinations With Highest Usage\n",
      "\n",
      "            destination  trip_average\n",
      "0                  loop         10727\n",
      "1           river_north          9523\n",
      "2         streeterville          6664\n",
      "3             west_loop          5163\n",
      "4                 ohare          2546\n",
      "5             lake_view          2420\n",
      "6            grant_park          2068\n",
      "7         museum_campus          1510\n",
      "8            gold_coast          1364\n",
      "9  sheffield_and_depaul          1259\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the top ten companies in comp_count_trip_df with the most rides\n",
    "top_ten_companies = comp_count_trip_df.nlargest(10, 'trip_sum')\n",
    "print(\"Ten Companies With Highest Usage\")\n",
    "print()\n",
    "print(top_ten_companies)\n",
    "print()\n",
    "\n",
    "# Top ten destinations\n",
    "    # Find the top ten destinations in dropoff_trip_avg_df with highest mean in 'trip_average'\n",
    "top_ten_destinations = dropoff_trip_avg_df.nlargest(10, 'trip_average').astype({'trip_average': 'int'})\n",
    "\n",
    "print(\"Ten Destinations With Highest Usage\")\n",
    "print()\n",
    "print(top_ten_destinations)\n",
    "print()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e373e5",
   "metadata": {},
   "source": [
    "With the top ten most used companies and top ten most popular destinations above, let's create two separate graphs. One will show total number of rides by taxi company, and the other will show top ten neighborhoods by number of dropoffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5905868e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp_count_trip_df written to comp_count_trip_df.csv\n",
      "dropoff_trip_avg_df written to dropoff_trip_avg_df.csv\n",
      "loop_ohare_df written to loop_ohare_df.csv\n",
      "loop_ohare_df written to loop_ohare_df.csv\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27f7075",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
